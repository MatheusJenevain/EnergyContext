{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Local PDF file path\n",
    "local_path = r\"D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\pdf\\appendixa_0.pdf\"\n",
    "\n",
    "# Load the PDF file\n",
    "loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "data = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a question generation pipeline\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "# Generate synthetic questions and answers\n",
    "synthetic_data = []\n",
    "for chunk in chunks:\n",
    "    text = chunk['content']\n",
    "    question = question_generator(\"generate question: \" + text)[0]['generated_text']\n",
    "    synthetic_data.append((question, text))\n",
    "\n",
    "# Separate into queries and expected answers\n",
    "test_queries, expected_answers = zip(*synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load the metrics\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "# Load a sentence transformer model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to evaluate the RAG system using BLEU, ROUGE, and semantic similarity\n",
    "def evaluate_rag_system(chain, test_queries, expected_answers):\n",
    "    predictions = [chain.invoke(query) for query in test_queries]\n",
    "    print(\"Predictions:\", predictions)  # Debug: Print predictions to see what the system returns\n",
    "\n",
    "    # Initialize lists to hold references and predictions for metrics calculation\n",
    "    references = [[ref.split()] for ref in expected_answers]  # BLEU expects list of lists of tokens\n",
    "    preds = [pred.split() for pred in predictions]\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_metric.add_batch(predictions=preds, references=references)\n",
    "    bleu_score = bleu_metric.compute()['bleu']\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_metric.add_batch(predictions=predictions, references=expected_answers)\n",
    "    rouge_scores = rouge_metric.compute()\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(predictions, expected_answers):\n",
    "        pred_embedding = model.encode(pred, convert_to_tensor=True)\n",
    "        ref_embedding = model.encode(ref, convert_to_tensor=True)\n",
    "        semantic_similarity = util.pytorch_cos_sim(pred_embedding, ref_embedding).item()\n",
    "        semantic_similarities.append(semantic_similarity)\n",
    "    \n",
    "    avg_semantic_similarity = np.mean(semantic_similarities)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"ROUGE-1 Score: {rouge_scores['rouge1'].mid.fmeasure:.2f}\")\n",
    "    print(f\"ROUGE-2 Score: {rouge_scores['rouge2'].mid.fmeasure:.2f}\")\n",
    "    print(f\"ROUGE-L Score: {rouge_scores['rougeL'].mid.fmeasure:.2f}\")\n",
    "    print(f\"Average Semantic Similarity: {avg_semantic_similarity:.2f}\")\n",
    "\n",
    "    return bleu_score, rouge_scores, avg_semantic_similarity\n",
    "\n",
    "# Evaluate the RAG system\n",
    "evaluate_rag_system(chain, test_queries, expected_answers)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
