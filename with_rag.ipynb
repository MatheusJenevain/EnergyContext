{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_path = r\"D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\2599.pdf\"\n",
    "local_path = r\"D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\pdf\\appendixa_0.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 3/3 [00:11<00:00,  3.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")'''\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database, and only the database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The provided text does not explicitly mention the technologies used to provide indoor environmental comfort, but it does discuss policies related to energy efficiency and renewable energy, which can contribute to improving indoor environmental quality by reducing emissions from fossil fuel-fired electric generating units and improving air quality. Some examples of technologies that can provide indoor environmental comfort are:\\n\\n1. Heating, Ventilation, and Air Conditioning (HVAC) systems - These systems control the temperature, humidity, and air quality in buildings by circulating air throughout the space, removing pollutants and contaminants, and providing heating or cooling as needed.\\n2. Energy-efficient lighting - LED lights are a popular choice for energy-efficient lighting solutions that reduce energy consumption and can improve indoor lighting quality.\\n3. Insulation materials - Proper insulation in buildings can help maintain comfortable temperatures, reduce heat loss or gain, and decrease the need for heating or cooling systems to run continuously.\\n4. Air purifiers - These devices can remove airborne particles, allergens, and pollutants, improving indoor air quality and overall comfort.\\n5. Smart building technologies - These technologies include sensors, controls, and automation systems that optimize energy use, temperature, lighting, and ventilation to ensure a comfortable indoor environment while minimizing energy consumption.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are technologies used to provide indoor environmental comfort?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1552 > 512). Running this sequence through the model will result in indexing errors\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a question generation pipeline\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "# Generate synthetic questions and answers\n",
    "synthetic_data = []\n",
    "for chunk in chunks:\n",
    "    # Assuming the text content is accessed via the 'page_content' attribute\n",
    "    text = chunk.page_content\n",
    "    question = question_generator(\"generate question: \" + text)[0]['generated_text']\n",
    "    synthetic_data.append((question, text))\n",
    "\n",
    "# Separate into queries and expected answers\n",
    "test_queries, expected_answers = zip(*synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus\\AppData\\Local\\Temp\\ipykernel_18412\\1570586788.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(\"bleu\")\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.41s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.24s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.45s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [' The term you are looking for isn\\'t explicitly stated in the provided text, but it seems to refer to a combination of \"State Implementation Plan (SIP)\" and \"Tribal Implementation Plan (TIP)\". These are plans that states and tribes submit to the U.S. Environmental Protection Agency (EPA) for review, which can include energy efficiency policies and programs. However, it\\'s not mentioned as a specific road map for incorporating energy efficiency in these plans. If I missed something, please let me know!', ' The document does not specify a specific organization that oversees the EE (Energy Efficiency) program, but it does mention the North American Electric Reliability Corporation (NERC) which ensures the reliability of the North American bulk power system. However, this is related to electric sector and not specifically Energy Efficiency programs.', ' The term you\\'re looking for is \"portfolio\" or \"program\". These terms can refer to a collection of energy efficiency projects with similar characteristics, installed in similar applications. For example, an energy efficiency program might include various measures such as high-efficiency appliances, efficient lighting, and advanced electric motor drives that are implemented across different sectors or market segments. Similarly, a portfolio could consist of a group of renewable energy projects like wind farms or solar arrays located in the same region and using similar technology.']\n",
      "BLEU Score: 0.00\n",
      "ROUGE-1 Score: 0.10\n",
      "ROUGE-2 Score: 0.04\n",
      "ROUGE-L Score: 0.06\n",
      "Average Semantic Similarity: 0.61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.415305365559244e-07,\n",
       " {'rouge1': AggregateScore(low=Score(precision=0.6395348837209303, recall=0.035422343324250684, fmeasure=0.06776715899218072), mid=Score(precision=0.6980778929672177, recall=0.05392628003744809, fmeasure=0.09910564961118339), high=Score(precision=0.7799999999999999, recall=0.07617728531855955, fmeasure=0.13613861386138612)),\n",
       "  'rouge2': AggregateScore(low=Score(precision=0.18823529411764706, recall=0.01818181818181818, fmeasure=0.034812880765883375), mid=Score(precision=0.29229545476727276, recall=0.020333673901526222, fmeasure=0.03764817362232107), high=Score(precision=0.40816326530612246, recall=0.022191400832177532, fmeasure=0.03970223325062035)),\n",
       "  'rougeL': AggregateScore(low=Score(precision=0.36046511627906974, recall=0.02633969118982743, fmeasure=0.05039096437880105), mid=Score(precision=0.4701148781171196, recall=0.034740738612536626, fmeasure=0.06405928316311015), high=Score(precision=0.58, recall=0.04293628808864266, fmeasure=0.07673267326732673)),\n",
       "  'rougeLsum': AggregateScore(low=Score(precision=0.5232558139534884, recall=0.03451407811080836, fmeasure=0.06602953953084276), mid=Score(precision=0.6285551508359017, recall=0.04721460510013836, fmeasure=0.08693950461370593), high=Score(precision=0.7600000000000001, recall=0.06232686980609418, fmeasure=0.11138613861386139))},\n",
       " 0.6054854989051819)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load the metrics\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "# Load a sentence transformer model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to evaluate the RAG system using BLEU, ROUGE, and semantic similarity\n",
    "def evaluate_rag_system(chain, test_queries, expected_answers):\n",
    "    predictions = [chain.invoke(query) for query in test_queries]\n",
    "    print(\"Predictions:\", predictions)  # Debug: Print predictions to see what the system returns\n",
    "\n",
    "    # Initialize lists to hold references and predictions for metrics calculation\n",
    "    references = [[ref.split()] for ref in expected_answers]  # BLEU expects list of lists of tokens\n",
    "    preds = [pred.split() for pred in predictions]\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_metric.add_batch(predictions=preds, references=references)\n",
    "    bleu_score = bleu_metric.compute()['bleu']\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_metric.add_batch(predictions=predictions, references=expected_answers)\n",
    "    rouge_scores = rouge_metric.compute()\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(predictions, expected_answers):\n",
    "        pred_embedding = model.encode(pred, convert_to_tensor=True)\n",
    "        ref_embedding = model.encode(ref, convert_to_tensor=True)\n",
    "        semantic_similarity = util.pytorch_cos_sim(pred_embedding, ref_embedding).item()\n",
    "        semantic_similarities.append(semantic_similarity)\n",
    "    \n",
    "    avg_semantic_similarity = np.mean(semantic_similarities)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"ROUGE-1 Score: {rouge_scores['rouge1'].mid.fmeasure:.2f}\")\n",
    "    print(f\"ROUGE-2 Score: {rouge_scores['rouge2'].mid.fmeasure:.2f}\")\n",
    "    print(f\"ROUGE-L Score: {rouge_scores['rougeL'].mid.fmeasure:.2f}\")\n",
    "    print(f\"Average Semantic Similarity: {avg_semantic_similarity:.2f}\")\n",
    "\n",
    "    return bleu_score, rouge_scores, avg_semantic_similarity\n",
    "\n",
    "# Evaluate the RAG system\n",
    "evaluate_rag_system(chain, test_queries, expected_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
