{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_path = r\"D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\2599.pdf\"\n",
    "local_path = r\"D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\pdf\\appendixa_0.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")'''\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an electrical engineer. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database, and only the database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide an answer combining the most important points of these five versions.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.77s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.03s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.20s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The document provided does not explicitly mention technologies for providing indoor environmental comfort. However, it does mention Combined Heat and Power (CHP), which can be a system that provides both heating and cooling for buildings, thus contributing to indoor environmental comfort. Additionally, High-efficiency heating, ventilating and air conditioning systems or control modifications are also mentioned as examples of energy efficiency, which is crucial for maintaining comfortable indoor environments. It's important to note that this is an inference from the information provided and not a direct mention of the technologies in the document.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are technologies used to provide indoor environmental comfort?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1552 > 512). Running this sequence through the model will result in indexing errors\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a question generation pipeline\n",
    "question_generator = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "# Generate synthetic questions and answers\n",
    "synthetic_data = []\n",
    "for chunk in chunks:\n",
    "    # Assuming the text content is accessed via the 'page_content' attribute\n",
    "    text = chunk.page_content\n",
    "    question = question_generator(\"generate question: \" + text)[0]['generated_text']\n",
    "    synthetic_data.append((question, text))\n",
    "\n",
    "# Separate into queries and expected answers\n",
    "test_queries, expected_answers = zip(*synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus\\AppData\\Local\\Temp\\ipykernel_14528\\1570586788.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(\"bleu\")\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.71s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.31s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.08s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.14s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.77s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.12s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.17s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.71s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.04s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.11s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [' The name of the roadmap for incorporating energy efficiency into state and tribal implementation plans is not explicitly mentioned in the provided text. However, it can be inferred that such a roadmap might be referred to as part of the \"On the books\" Energy Efficiency/ Renewable Energy Policies or \"On the way\" Energy Efficiency/ Renewable Energy Policies, which are policies that have been adopted or planned for adoption by a legislative or regulatory body. The actual name of this roadmap would depend on the specific jurisdiction and context in which it is being implemented.', \" The document does not provide specific information about an organization that oversees the Energy Efficiency (EE) program directly. However, the North American Electric Reliability Corporation (NERC) is mentioned as an organization that ensures the reliability of the North American bulk power system, but this doesn't necessarily mean they oversee EE programs. The U.S. Environmental Protection Agency (EPA) plays a significant role in regulating various aspects related to energy efficiency and renewable energy policies.\", ' The name of the group of projects that are similar in characteristics and installed in similar applications, as described in the provided text, is not explicitly stated. However, it can be inferred that they might be referred to as \"Energy Efficiency Programs\" or \"End-use sectors (or specific market segments within a sector)\" based on their purpose of increasing adoption of energy efficient technologies and practices.']\n",
      "BLEU Score: 0.00\n",
      "ROUGE-1 Score: 0.11\n",
      "ROUGE-2 Score: 0.05\n",
      "ROUGE-L Score: 0.07\n",
      "Average Semantic Similarity: 0.59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.1556046581504414e-06,\n",
       " {'rouge1': AggregateScore(low=Score(precision=0.6818181818181818, recall=0.04904632152588556, fmeasure=0.09183673469387756), mid=Score(precision=0.7154996776273371, recall=0.058032401889632435, fmeasure=0.10725080400075455), high=Score(precision=0.7446808510638298, recall=0.06272401433691756, fmeasure=0.11570247933884296)),\n",
       "  'rouge2': AggregateScore(low=Score(precision=0.2680452164323132, recall=0.023463923424740175, fmeasure=0.04314606047329369), mid=Score(precision=0.31662456501166175, recall=0.024854668601893126, fmeasure=0.046043153699008306), high=Score(precision=0.3918918918918919, recall=0.026363636363636363, fmeasure=0.04940374787052811)),\n",
       "  'rougeL': AggregateScore(low=Score(precision=0.4393939393939394, recall=0.03178928247048138, fmeasure=0.059523809523809514), mid=Score(precision=0.4651407693960885, recall=0.03772470848315417, fmeasure=0.06972030942958492), high=Score(precision=0.48936170212765956, recall=0.04121863799283154, fmeasure=0.07603305785123966)),\n",
       "  'rougeLsum': AggregateScore(low=Score(precision=0.6212121212121212, recall=0.047229791099000905, fmeasure=0.08843537414965985), mid=Score(precision=0.6474016763378465, recall=0.05229462607088458, fmeasure=0.09667231633742279), high=Score(precision=0.6933333333333334, recall=0.056786703601108025, fmeasure=0.10406091370558375))},\n",
       " 0.5851089954376221)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Load the metrics\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "# Load a sentence transformer model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to evaluate the RAG system using BLEU, ROUGE, and semantic similarity\n",
    "def evaluate_rag_system(chain, test_queries, expected_answers):\n",
    "    predictions = [chain.invoke(query) for query in test_queries]\n",
    "    print(\"Predictions:\", predictions)  # Debug: Print predictions to see what the system returns\n",
    "\n",
    "    # Initialize lists to hold references and predictions for metrics calculation\n",
    "    references = [[ref.split()] for ref in expected_answers]  # BLEU expects list of lists of tokens\n",
    "    preds = [pred.split() for pred in predictions]\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_metric.add_batch(predictions=preds, references=references)\n",
    "    bleu_score = bleu_metric.compute()['bleu']\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_metric.add_batch(predictions=predictions, references=expected_answers)\n",
    "    rouge_scores = rouge_metric.compute()\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    semantic_similarities = []\n",
    "    for pred, ref in zip(predictions, expected_answers):\n",
    "        pred_embedding = model.encode(pred, convert_to_tensor=True)\n",
    "        ref_embedding = model.encode(ref, convert_to_tensor=True)\n",
    "        semantic_similarity = util.pytorch_cos_sim(pred_embedding, ref_embedding).item()\n",
    "        semantic_similarities.append(semantic_similarity)\n",
    "    \n",
    "    avg_semantic_similarity = np.mean(semantic_similarities)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"ROUGE-1 Score: {rouge_scores['rouge1'].mid.fmeasure:.2f}\")\n",
    "    print(f\"ROUGE-2 Score: {rouge_scores['rouge2'].mid.fmeasure:.2f}\")\n",
    "    print(f\"ROUGE-L Score: {rouge_scores['rougeL'].mid.fmeasure:.2f}\")\n",
    "    print(f\"Average Semantic Similarity: {avg_semantic_similarity:.2f}\")\n",
    "\n",
    "    return bleu_score, rouge_scores, avg_semantic_similarity\n",
    "\n",
    "# Evaluate the RAG system\n",
    "evaluate_rag_system(chain, test_queries, expected_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.68s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.07s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.11s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.20s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.09s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.15s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.17s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.27s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.08s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.10s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "OllamaEmbeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.22s/it]\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [' The Megawatt (MW) is a unit of power, representing one million watts (1 Joule per second). On the other hand, the Megawatt-hour (MWh) is a unit of energy, representing one million watt-hours (1 Joule per second for 3600 seconds or 3.6 kilowatt-hours). In simpler terms, 1 MW is the amount of power that can produce 1 MWh of energy over a period of one hour.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.14\n",
      "Average ROUGE-1 Score: 0.50\n",
      "Average ROUGE-2 Score: 0.32\n",
      "Average ROUGE-L Score: 0.44\n",
      "Average Semantic Similarity: 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1390357706150399,\n",
       " 0.4954128440366973,\n",
       " 0.3177570093457944,\n",
       " 0.4403669724770642,\n",
       " 0.9627901315689087)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Define a set of handwritten test queries and expected answers\n",
    "test_queries = [\n",
    "    \"What is the relationship between Megawatt (MW) and Megawatt-hour (MWh)?\",\n",
    "    # Add more handwritten test queries here\n",
    "]\n",
    "\n",
    "expected_answers = [\n",
    "    \"A megawatt (MW) is a unit of power representing the rate at which energy is used or generated, while a megawatt-hour (MWh) is a unit of energy representing the total amount of energy used or generated over an hour.\",\n",
    "    # Add more expected answers corresponding to the handwritten test queries\n",
    "]\n",
    "\n",
    "# Function to evaluate the RAG system using BLEU, ROUGE, and semantic similarity\n",
    "def evaluate_rag_system(chain, test_queries, expected_answers):\n",
    "    # Get predictions from the RAG system\n",
    "    predictions = [chain.invoke(query) for query in test_queries]\n",
    "    print(\"Predictions:\", predictions)  # Debug: Print predictions to see what the system returns\n",
    "\n",
    "    # Initialize metrics\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    semantic_similarities = []\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Load a sentence transformer model for semantic similarity\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    for pred, ref in zip(predictions, expected_answers):\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([ref.split()], pred.split(), smoothing_function=SmoothingFunction().method1)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_score = rouge.score(ref, pred)\n",
    "        rouge_scores.append(rouge_score)\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        pred_embedding = model.encode(pred, convert_to_tensor=True)\n",
    "        ref_embedding = model.encode(ref, convert_to_tensor=True)\n",
    "        semantic_similarity = util.pytorch_cos_sim(pred_embedding, ref_embedding).item()\n",
    "        semantic_similarities.append(semantic_similarity)\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "    avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "    avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "    avg_semantic_similarity = np.mean(semantic_similarities)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.2f}\")\n",
    "    print(f\"Average ROUGE-1 Score: {avg_rouge1:.2f}\")\n",
    "    print(f\"Average ROUGE-2 Score: {avg_rouge2:.2f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rougeL:.2f}\")\n",
    "    print(f\"Average Semantic Similarity: {avg_semantic_similarity:.2f}\")\n",
    "\n",
    "    return avg_bleu, avg_rouge1, avg_rouge2, avg_rougeL, avg_semantic_similarity\n",
    "\n",
    "# Assuming 'chain' is your RAG system already defined\n",
    "# For example, chain.invoke(\"Your query\") should return the answer from the RAG system\n",
    "evaluate_rag_system(chain, test_queries, expected_answers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
