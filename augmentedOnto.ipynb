{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from owlready2 import get_ontology\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ontology...\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "directory = 'D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\ontology\\imports'\n",
    "filename = 'oec-extracted.owl'\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "print(\"Loading ontology...\")\n",
    "# Load the ontology\n",
    "onto = get_ontology(\"file://\" + file_path).load()\n",
    "\n",
    "# Define the IRIs (unique identifiers) for properties and individual in the ontology\n",
    "term_sent_by_property_iri = \"http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted#termSentBy\"\n",
    "actor_coal_iri = \"http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted#actorCoal\"\n",
    "actor_has_context_property_iri = \"http://www.semanticweb.org/matheus/ontologies/2023/10/oec-extracted#actorHasContext\"\n",
    "\n",
    "# Get the ontology elements (properties and individual) using their IRIs\n",
    "term_sent_by_property = onto.search_one(iri=term_sent_by_property_iri)\n",
    "actor_coal = onto.search_one(iri=actor_coal_iri)\n",
    "actor_has_context_property = onto.search_one(iri=actor_has_context_property_iri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 3/3 [00:09<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF file\n",
    "local_path = r\"D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\pdf\\appendixa_0.pdf\"\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "else:\n",
    "    raise FileNotFoundError(\"Upload a PDF file\")\n",
    "\n",
    "# Split PDF into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Create the vector database using the Ollama Embeddings model\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template for querying the context\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"answer_prompt_template = You are an expert in electrial engineering.\n",
    "    Using only the information provided in the context, choose the best answer, and only that, to the following question:\n",
    "    Original question: {question}\"\"\")\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a template for formatting the question and context\n",
    "template = \"\"\"Answer the question based ONLY on the following context with ONLY one answer:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Set up a chain to retrieve relevant context, format the question and answer using the template, and generate a response using the language model\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termMeaningString already exists for net metering. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Storage?\n",
      "Storage :  Storage refers to the technology or infrastructure used to store energy generated from renewable sources (such as solar, wind, geothermal) for later use. This can help balance supply and demand in the electricity grid by storing excess energy during periods of low consumption and releasing it when demand is high. The type of storage varies, such as batteries, pumped hydroelectric systems, or thermal storage systems like molten salt or ice-based storage.\n"
     ]
    }
   ],
   "source": [
    "# Define default ontology path for saving (use raw string or double backslashes)\n",
    "default_save_path = r'D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\ontology\\imports\\updated_oec-extracted.owl'\n",
    "file_path = r'D:\\GitHub\\Projetos\\Mestrado\\EnergyContext\\ontology\\imports\\oec-extracted.owl'\n",
    "\n",
    "# List to store individual IRIs to be updated\n",
    "individuals_to_update = []\n",
    "\n",
    "# First Loop: Collect Individuals to Update\n",
    "if term_sent_by_property and actor_coal and actor_has_context_property:\n",
    "    for individual in onto.individuals():\n",
    "        if term_sent_by_property in individual.get_properties():\n",
    "            term_sent_by_values = getattr(individual, term_sent_by_property.python_name)\n",
    "            if actor_coal in term_sent_by_values:\n",
    "                # Ensure termLexiconString is a list\n",
    "                if hasattr(individual, \"termLexiconString\"):\n",
    "                    if not isinstance(individual.termLexiconString, list):\n",
    "                        individual.termLexiconString = [individual.termLexiconString]\n",
    "                individuals_to_update.append(individual)  # Add the individual object itself\n",
    "\n",
    "# Second Loop: Perform Question-Answering and Update Ontology\n",
    "for individual in individuals_to_update:\n",
    "    with onto:  \n",
    "        try: \n",
    "            if not individual:\n",
    "                print(f\"Warning: Individual with IRI {individual.iri} not found.\")\n",
    "                continue  # Skip to the next individual if not found\n",
    "\n",
    "            if hasattr(individual, \"termLexiconString\"):\n",
    "                term_lexicon_string_value = getattr(individual, \"termLexiconString\")\n",
    "                if isinstance(term_lexicon_string_value, list):\n",
    "                    term_lexicon_string_value = \" \".join(term_lexicon_string_value)\n",
    "\n",
    "                # Check if termLexiconString is empty after joining\n",
    "                if term_lexicon_string_value.strip():  \n",
    "                    lexicon_question = \"What is \" + term_lexicon_string_value + \"?\"\n",
    "\n",
    "                    # Check if termMeaningString exists and is not empty\n",
    "                    if hasattr(individual, 'termMeaningString') and individual.termMeaningString:\n",
    "                        print(f\"termMeaningString already exists for {term_lexicon_string_value}. Skipping.\")\n",
    "                    else:\n",
    "                        answer = chain.invoke(lexicon_question)\n",
    "                        print(\"Question:\", lexicon_question)\n",
    "                        print(term_lexicon_string_value, \":\", answer)\n",
    "                        # Update the termMeaningString property\n",
    "                        individual.termMeaningString.append(answer)\n",
    "                else:\n",
    "                    print(\" termLexiconString is empty after joining\")\n",
    "            else:\n",
    "                print(\" termLexiconString: Not found\")\n",
    "        except Exception as e:  # Catch any unexpected errors\n",
    "            print(f\"Error processing individual {individual.iri}: {e}\")\n",
    "    # Save the updated ontology (only once after each individual is updated)\n",
    "    onto.save(file_path)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
